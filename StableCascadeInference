{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Stability-AI/StableCascade","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:07:23.756630Z","iopub.execute_input":"2024-05-26T18:07:23.757530Z","iopub.status.idle":"2024-05-26T18:07:32.092094Z","shell.execute_reply.started":"2024-05-26T18:07:23.757492Z","shell.execute_reply":"2024-05-26T18:07:32.090926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv StableCascade/* .","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:07:32.094096Z","iopub.execute_input":"2024-05-26T18:07:32.094412Z","iopub.status.idle":"2024-05-26T18:07:33.035796Z","shell.execute_reply.started":"2024-05-26T18:07:32.094381Z","shell.execute_reply":"2024-05-26T18:07:33.034730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!models/download_models.sh essential small-small bfloat16\n#!models/download_models.sh essential big-big bfloat16\n!wget https://huggingface.co/stabilityai/StableWurst/resolve/main/stage_b_lite.safetensors -P . -q --show-progress\n!wget https://huggingface.co/stabilityai/StableWurst/resolve/main/stage_c_lite.safetensors -P . -q --show-progress\n!wget https://huggingface.co/stabilityai/StableWurst/resolve/main/stage_a.safetensors -P . -q --show-progress\n!wget https://huggingface.co/stabilityai/StableWurst/resolve/main/previewer.safetensors -P . -q --show-progress\n!wget https://huggingface.co/stabilityai/StableWurst/resolve/main/effnet_encoder.safetensors -P . -q --show-progress","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:07:33.037449Z","iopub.execute_input":"2024-05-26T18:07:33.037853Z","iopub.status.idle":"2024-05-26T18:12:19.170146Z","shell.execute_reply.started":"2024-05-26T18:07:33.037810Z","shell.execute_reply":"2024-05-26T18:12:19.169098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:12:19.173276Z","iopub.execute_input":"2024-05-26T18:12:19.173694Z","iopub.status.idle":"2024-05-26T18:12:20.140409Z","shell.execute_reply.started":"2024-05-26T18:12:19.173649Z","shell.execute_reply":"2024-05-26T18:12:20.139470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!rm ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:12:20.142141Z","iopub.execute_input":"2024-05-26T18:12:20.142520Z","iopub.status.idle":"2024-05-26T18:12:21.093751Z","shell.execute_reply.started":"2024-05-26T18:12:20.142482Z","shell.execute_reply":"2024-05-26T18:12:21.092828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv stage_a.safetensors models\n!mv previewer.safetensors models\n!mv effnet_encoder.safetensors models","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:12:21.095148Z","iopub.execute_input":"2024-05-26T18:12:21.095464Z","iopub.status.idle":"2024-05-26T18:12:23.909716Z","shell.execute_reply.started":"2024-05-26T18:12:21.095435Z","shell.execute_reply":"2024-05-26T18:12:23.908498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv stage_c_lite.safetensors models\n!mv stage_b_lite.safetensors models","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:12:23.911295Z","iopub.execute_input":"2024-05-26T18:12:23.911598Z","iopub.status.idle":"2024-05-26T18:12:25.781564Z","shell.execute_reply.started":"2024-05-26T18:12:23.911569Z","shell.execute_reply":"2024-05-26T18:12:25.780291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -r requirements.txt","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-26T18:12:25.783296Z","iopub.execute_input":"2024-05-26T18:12:25.783680Z","iopub.status.idle":"2024-05-26T18:16:16.857140Z","shell.execute_reply.started":"2024-05-26T18:12:25.783640Z","shell.execute_reply":"2024-05-26T18:16:16.856032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport yaml\nimport torch\nfrom tqdm import tqdm\n\n#os.chdir('..')\nfrom inference.utils import *\nfrom core.utils import load_or_fail\nfrom train import WurstCoreC, WurstCoreB\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:27:10.377588Z","iopub.status.idle":"2024-05-26T18:27:10.377946Z","shell.execute_reply.started":"2024-05-26T18:27:10.377748Z","shell.execute_reply":"2024-05-26T18:27:10.377774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:16:25.319799Z","iopub.execute_input":"2024-05-26T18:16:25.320263Z","iopub.status.idle":"2024-05-26T18:16:26.280860Z","shell.execute_reply.started":"2024-05-26T18:16:25.320236Z","shell.execute_reply":"2024-05-26T18:16:26.279798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the content for the YAML file\nyaml_content = \"\"\"# GLOBAL STUFF\nmodel_version: 700M\ndtype: float32\n\n# For demonstration purposes in reconstruct_images.ipynb\nwebdataset_path: file:inference/imagenet_1024.tar\nbatch_size: 1\nimage_size: 1024\ngrad_accum_steps: 1\n\neffnet_checkpoint_path: models/effnet_encoder.safetensors\nstage_a_checkpoint_path: models/stage_a.safetensors\ngenerator_checkpoint_path: models/stage_b_lite.safetensors\n\"\"\"\n\n# Define the file path\nfile_path = \"configs/inference/stage_b_lite.yaml\"\n\n# Ensure the directories exist\nos.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n# Write the content to the file\nwith open(file_path, 'w') as file:\n    file.write(yaml_content)\n    \nimport os\n\n# Define the content for the YAML file\nyaml_content = \"\"\"# GLOBAL STUFF\nmodel_version: 1B\ndtype: float32\n\neffnet_checkpoint_path: models/effnet_encoder.safetensors\npreviewer_checkpoint_path: models/previewer.safetensors\ngenerator_checkpoint_path: models/stage_c_lite.safetensors\n\"\"\"\n\n# Define the file path\nfile_path = \"configs/inference/stage_c_lite.yaml\"\n\n# Ensure the directories exist\nos.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n# Write the content to the file\nwith open(file_path, 'w') as file:\n    file.write(yaml_content)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:16:26.282337Z","iopub.execute_input":"2024-05-26T18:16:26.282688Z","iopub.status.idle":"2024-05-26T18:16:26.291734Z","shell.execute_reply.started":"2024-05-26T18:16:26.282656Z","shell.execute_reply":"2024-05-26T18:16:26.290839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Config","metadata":{}},{"cell_type":"code","source":"# SETUP STAGE C\nconfig_file = 'configs/inference/stage_c_lite.yaml'\n#config_file = 'configs/inference/stage_c.yaml'\nwith open(config_file, \"r\", encoding=\"utf-8\") as file:\n    loaded_config = yaml.safe_load(file)\n\ncore = WurstCoreC(config_dict=loaded_config, device=device, training=False)\n\n# SETUP STAGE B\nconfig_file_b = 'configs/inference/stage_b_lite.yaml'\n#config_file_b = 'configs/inference/stage_b.yaml'\nwith open(config_file_b, \"r\", encoding=\"utf-8\") as file:\n    config_file_b = yaml.safe_load(file)\n    \ncore_b = WurstCoreB(config_dict=config_file_b, device=device, training=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:16:26.292887Z","iopub.execute_input":"2024-05-26T18:16:26.293159Z","iopub.status.idle":"2024-05-26T18:16:26.310353Z","shell.execute_reply.started":"2024-05-26T18:16:26.293134Z","shell.execute_reply":"2024-05-26T18:16:26.309475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Extras & Models","metadata":{}},{"cell_type":"code","source":"# SETUP MODELS & DATA\nextras = core.setup_extras_pre()\nmodels = core.setup_models(extras)\nmodels.generator.eval().requires_grad_(False)\nprint(\"STAGE C READY\")\n\nextras_b = core_b.setup_extras_pre()\nmodels_b = core_b.setup_models(extras_b, skip_clip=True)\nmodels_b = WurstCoreB.Models(\n   **{**models_b.to_dict(), 'tokenizer': models.tokenizer, 'text_model': models.text_model}\n)\nmodels_b.generator.eval().requires_grad_(False) #.bfloat16().\nprint(\"STAGE B READY\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:16:26.311509Z","iopub.execute_input":"2024-05-26T18:16:26.311820Z","iopub.status.idle":"2024-05-26T18:26:18.322641Z","shell.execute_reply.started":"2024-05-26T18:16:26.311795Z","shell.execute_reply":"2024-05-26T18:26:18.321492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optional: Compile Stage C and Stage B","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"**Note**: This will increase speed inference by about 2x, but will initially take a few minutes to compile. Moreover, currently using `torch.compile` only works for a single image resolution, e.g. 1024 x 1024. If you use a different size, it will recompile. See more [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assert False, \"STOP HERE\"\n#models = WurstCoreC.Models(\n#   **{**models.to_dict(), 'generator': torch.compile(models.generator, mode=\"reduce-overhead\", fullgraph=True)}\n#)\n\n#models_b = WurstCoreB.Models(\n#   **{**models_b.to_dict(), 'generator': torch.compile(models_b.generator, mode=\"reduce-overhead\", fullgraph=True)}\n#)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:26:18.323832Z","iopub.execute_input":"2024-05-26T18:26:18.324128Z","iopub.status.idle":"2024-05-26T18:26:18.328378Z","shell.execute_reply.started":"2024-05-26T18:26:18.324101Z","shell.execute_reply":"2024-05-26T18:26:18.327430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text-to-Image","metadata":{}},{"cell_type":"code","source":"batch_size = 1\n# caption = \"Cinematic photo of an anthropomorphic nerdy rodent sitting in a cafe reading a book\"\ncaption = \"fashionable zara model, realistic, full shot\"\nheight, width = 1024, 1024\nstage_c_latent_shape, stage_b_latent_shape = calculate_latent_sizes(height, width, batch_size=batch_size)\n\n# Stage C Parameters\nextras.sampling_configs['cfg'] = 4\nextras.sampling_configs['shift'] = 2\nextras.sampling_configs['timesteps'] = 20\nextras.sampling_configs['t_start'] = 1.0\n\n# Stage B Parameters\nextras_b.sampling_configs['cfg'] = 1.1\nextras_b.sampling_configs['shift'] = 1\nextras_b.sampling_configs['timesteps'] = 20\nextras_b.sampling_configs['t_start'] = 1.0\n\n# PREPARE CONDITIONS\n\nbatch = {'captions': [caption] * batch_size}\nconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\nunconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \nconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False)\nunconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True)\n\n#assert str(device)=='cpu', \"for non cpu change to torch.cuda.amp.autocast(dtype=torch.bfloat16)\"\nwith torch.no_grad():#, torch.cuda.amp.autocast(dtype=torch.bfloat16):#torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    # torch.manual_seed(42)\n\n    sampling_c = extras.gdf.sample(\n        models.generator, conditions, stage_c_latent_shape,\n        unconditions, device=device, **extras.sampling_configs,\n    )\n    for (sampled_c, _, _) in tqdm(sampling_c, total=extras.sampling_configs['timesteps']):\n        sampled_c = sampled_c\n        \n    # preview_c = models.previewer(sampled_c).float()\n    # show_images(preview_c)\n\n    conditions_b['effnet'] = sampled_c\n    unconditions_b['effnet'] = torch.zeros_like(sampled_c)\n\n    sampling_b = extras_b.gdf.sample(\n        models_b.generator, conditions_b, stage_b_latent_shape,\n        unconditions_b, device=device, **extras_b.sampling_configs\n    )\n    for (sampled_b, _, _) in tqdm(sampling_b, total=extras_b.sampling_configs['timesteps']):\n        sampled_b = sampled_b\n    sampled = models_b.stage_a.decode(sampled_b).float()\n\nshow_images(sampled)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:34:17.279245Z","iopub.execute_input":"2024-05-26T18:34:17.280068Z","iopub.status.idle":"2024-05-26T18:35:26.012414Z","shell.execute_reply.started":"2024-05-26T18:34:17.280032Z","shell.execute_reply":"2024-05-26T18:35:26.011431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Variation","metadata":{}},{"cell_type":"code","source":"batch_size = 4\n# url = \"https://media.discordapp.net/attachments/1121232062708457508/1204613422680113212/1707272583_2.png?ex=65d55eac&is=65c2e9ac&hm=7741ea0f494b04c830128d883d7f03b28b5caffb85959d3fc0a0fb3d18d0411e&=&format=webp&quality=lossless\"\nurl = \"https://media.discordapp.net/attachments/1121232062708457508/1205134776206491648/image.png?ex=65d74438&is=65c4cf38&hm=fcb40fc6bbe437dee481afffcd94e25c5511d059341b2f2b6e046f157e6b9371&=&format=webp&quality=lossless\"\nimages = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device)\n\nbatch = {'images': images}\n\nshow_images(batch['images'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:27:09.201037Z","iopub.execute_input":"2024-05-26T18:27:09.201326Z","iopub.status.idle":"2024-05-26T18:27:10.370858Z","shell.execute_reply.started":"2024-05-26T18:27:09.201301Z","shell.execute_reply":"2024-05-26T18:27:10.369513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption = \"\"\nheight, width = 1024, 1024\nstage_c_latent_shape, stage_b_latent_shape = calculate_latent_sizes(height, width, batch_size=batch_size)\n\n# Stage C Parameters\nextras.sampling_configs['cfg'] = 4\nextras.sampling_configs['shift'] = 2\nextras.sampling_configs['timesteps'] = 20\nextras.sampling_configs['t_start'] = 1.0\n\n# Stage B Parameters\nextras_b.sampling_configs['cfg'] = 1.1\nextras_b.sampling_configs['shift'] = 1\nextras_b.sampling_configs['timesteps'] = 10\nextras_b.sampling_configs['t_start'] = 1.0\n\n# PREPARE CONDITIONS\nbatch['captions'] = [caption] * batch_size\n\nwith torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n    # torch.manual_seed(42)\n    conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=True)\n    unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n    conditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False)\n    unconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True)\n\n    sampling_c = extras.gdf.sample(\n        models.generator, conditions, stage_c_latent_shape,\n        unconditions, device=device, **extras.sampling_configs,\n    )\n    for (sampled_c, _, _) in tqdm(sampling_c, total=extras.sampling_configs['timesteps']):\n        sampled_c = sampled_c\n        \n    # preview_c = models.previewer(sampled_c).float()\n    # show_images(preview_c)\n\n    conditions_b['effnet'] = sampled_c\n    unconditions_b['effnet'] = torch.zeros_like(sampled_c)\n\n    sampling_b = extras_b.gdf.sample(\n        models_b.generator, conditions_b, stage_b_latent_shape,\n        unconditions_b, device=device, **extras_b.sampling_configs\n    )\n    for (sampled_b, _, _) in tqdm(sampling_b, total=extras_b.sampling_configs['timesteps']):\n        sampled_b = sampled_b\n    sampled = models_b.stage_a.decode(sampled_b).float()\n\nshow_images(sampled)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:27:10.372154Z","iopub.status.idle":"2024-05-26T18:27:10.372664Z","shell.execute_reply.started":"2024-05-26T18:27:10.372404Z","shell.execute_reply":"2024-05-26T18:27:10.372424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image-to-Image","metadata":{}},{"cell_type":"code","source":"batch_size = 4\nurl = \"https://media.discordapp.net/attachments/1121232062708457508/1204557773480656947/chrome_rodent_knight.png?ex=65d52ad8&is=65c2b5d8&hm=8e74f16e685e54a4f67337fedbdfea350169d03babc6c2f79fd1b74a6fb665fb&=&format=webp&quality=lossless\"\nimages = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device)\n\nbatch = {'images': images}\n\nshow_images(batch['images'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:27:10.374243Z","iopub.status.idle":"2024-05-26T18:27:10.374595Z","shell.execute_reply.started":"2024-05-26T18:27:10.374425Z","shell.execute_reply":"2024-05-26T18:27:10.374441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption = \"a person riding a rodent\"\nnoise_level = 0.8\nheight, width = 1024, 1024\nstage_c_latent_shape, stage_b_latent_shape = calculate_latent_sizes(height, width, batch_size=batch_size)\n\neffnet_latents = core.encode_latents(batch, models, extras)\nt = torch.ones(effnet_latents.size(0), device=device) * noise_level\nnoised = extras.gdf.diffuse(effnet_latents, t=t)[0]\n\n# Stage C Parameters\nextras.sampling_configs['cfg'] = 4\nextras.sampling_configs['shift'] = 2\nextras.sampling_configs['timesteps'] = int(20 * noise_level)\nextras.sampling_configs['t_start'] = noise_level\nextras.sampling_configs['x_init'] = noised\n\n# Stage B Parameters\nextras_b.sampling_configs['cfg'] = 1.1\nextras_b.sampling_configs['shift'] = 1\nextras_b.sampling_configs['timesteps'] = 10\nextras_b.sampling_configs['t_start'] = 1.0\n\n# PREPARE CONDITIONS\nbatch['captions'] = [caption] * batch_size\n\nwith torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n    # torch.manual_seed(42)\n    conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n    unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n    conditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False)\n    unconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True)\n\n    sampling_c = extras.gdf.sample(\n        models.generator, conditions, stage_c_latent_shape,\n        unconditions, device=device, **extras.sampling_configs,\n    )\n    for (sampled_c, _, _) in tqdm(sampling_c, total=extras.sampling_configs['timesteps']):\n        sampled_c = sampled_c\n        \n    # preview_c = models.previewer(sampled_c).float()\n    # show_images(preview_c)\n\n    conditions_b['effnet'] = sampled_c\n    unconditions_b['effnet'] = torch.zeros_like(sampled_c)\n\n    sampling_b = extras_b.gdf.sample(\n        models_b.generator, conditions_b, stage_b_latent_shape,\n        unconditions_b, device=device, **extras_b.sampling_configs\n    )\n    for (sampled_b, _, _) in tqdm(sampling_b, total=extras_b.sampling_configs['timesteps']):\n        sampled_b = sampled_b\n    sampled = models_b.stage_a.decode(sampled_b).float()\n\nshow_images(batch['images'])\nshow_images(sampled)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:27:10.376276Z","iopub.status.idle":"2024-05-26T18:27:10.376626Z","shell.execute_reply.started":"2024-05-26T18:27:10.376460Z","shell.execute_reply":"2024-05-26T18:27:10.376475Z"},"trusted":true},"execution_count":null,"outputs":[]}]}